---
---

@string{aps = {American Physical Society,}}
@article{Gao2024,
abstract = {Reconstructing a 3D shape based on a single sketch image is challenging due to the inherent sparsity and ambiguity present in sketches. Existing methods lose fine details when extracting features to predict 3D objects from sketches. Upon analyzing the 3D-to-2D projection process, we observe that the density map, characterizing the distribution of 2D point clouds, can serve as a proxy to facilitate the reconstruction process. In this work, we propose a novel sketch-based 3D reconstruction model named <italic>SketchSampler</italic>. It initiates the process by translating a sketch through an image translation network into a more informative 2D representation, which is then used to generate a density map. Subsequently, a two-stage probabilistic sampling process is employed to reconstruct a 3D point cloud: firstly, recovering the 2D points (i.e., the <inline-formula><tex-math notation="LaTeX">$x$</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX">$y$</tex-math></inline-formula> coordinates) by sampling the density map; and secondly, predicting the depth (i.e., the <inline-formula><tex-math notation="LaTeX">$z$</tex-math></inline-formula> coordinate) by sampling the depth values along the ray determined by each 2D point. Additionally, we convert the reconstructed point cloud into a 3D mesh for wider applications. To reduce ambiguity, we incorporate hidden lines in sketches. Experimental results demonstrate that our proposed approach significantly outperforms other baseline methods.},
author = {Gao, Chenjian and Wang, Xilin and Yu, Qian and Sheng, Lu and Zhang, Jing and Han, Xiaoguang and Song, Yi Zhe and Xu, Dong},
doi = {10.1109/TPAMI.2024.3424404},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2024_TPAMI-2024-01-0043_Proof_hi.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Feature extraction,Image reconstruction,Point cloud compression,Shape,Solid modeling,Surface reconstruction,Three-dimensional displays},
title = {{3D Reconstruction from a Single Sketch via View-dependent Depth Sampling}},
year = {2024}
}
@inproceedings{Wu2024,
abstract = {NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website https://zkaiwu.github.io/RaFE-Project/.},
archivePrefix = {arXiv},
arxivId = {2404.03654},
author = {Wu, Zhongkai and Wan, Ziyu and Zhang, Jing and Liao, Jing and Xu, Dong},
booktitle = {European Conference on Computer Vision},
eprint = {2404.03654},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2024_RaFE-Generative Radiance Fields Restoration.pdf:pdf},
keywords = {3d restoration,generative model,neural radiance fields,neural rendering},
title = {{RaFE: Generative Radiance Fields Restoration}},
url = {http://arxiv.org/abs/2404.03654},
year = {2024}
}

@inproceedings{Li2024,
abstract = {Panoptic narrative grounding (PNG), whose core target is fine-grained image-text alignment, requires a panoptic segmentation of referred objects given a narrative caption. Previous discriminative methods achieve only weak or coarse-grained alignment by panoptic segmentation pretraining or CLIP model adaptation. Given the recent progress of text-to-image Diffusion models, several works have shown their capability to achieve fine-grained image-text alignment through cross-attention maps and improved general segmentation performance. However, the direct use of phrase features as static prompts to apply frozen Diffusion models to the PNG task still suffers from a large task gap and insufficient vision-language interaction, yielding inferior performance. Therefore, we propose an Extractive-Injective Phrase Adapter (EIPA) bypass within the Diffusion UNet to dynamically update phrase prompts with image features and inject the multimodal cues back, which leverages the fine-grained image-text alignment capability of Diffusion models more sufficiently. In addition, we also design a Multi-Level Mutual Aggregation (MLMA) module to reciprocally fuse multi-level image and phrase features for segmentation refinement. Extensive experiments on the PNG benchmark show that our method achieves new state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {2409.08251},
author = {Li, Hongyu and Hui, Tianrui and Ding, Zihan and Zhang, Jing and Ma, Bin and Wei, Xiaoming and Han, Jizhong and Liu, Si},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia (MM '24)},
doi = {10.1145/3664647.3686836},
eprint = {2409.08251},
file = {:Users/jingzhang/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2024 - Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding.pdf:pdf},
isbn = {9798400706868},
keywords = {Diffusion Models,Dy,Panoptic Narrative Grounding,acm reference format,diffusion models,dynamic prompt-,ing,multi-level aggregation,panoptic narrative grounding,phrase adapter},
number = {1},
publisher = {Association for Computing Machinery},
title = {{Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding}},
url = {http://arxiv.org/abs/2409.08251},
volume = {1},
year = {2024}
}
@article{Tang2024,
abstract = {Deep learning models have the ability to extract rich knowledge from large-scale datasets. However, the sharing of data has become increasingly challenging due to concerns regarding data copyright and privacy. Consequently, this hampers the effective transfer of knowledge from existing data to novel downstream tasks and concepts. Zero-shot learning (ZSL) approaches aim to recognize new classes by transferring semantic knowledge learned from base classes. However, traditional generative ZSL methods often require access to real images from base classes and rely on manually annotated attributes, which presents challenges in terms of data restrictions and model scalability. To this end, this paper tackles a challenging and practical problem dubbed as data-free zero-shot learning (DFZSL), where only the CLIP-based base classes data pre-trained classifier is available for zero-shot classification. Specifically, we propose a generic framework for DFZSL, which consists of three main components. Firstly, to recover the virtual features of the base data, we model the CLIP features of base class images as samples from a von Mises-Fisher (vMF) distribution based on the pre-trained classifier. Secondly, we leverage the text features of CLIP as low-cost semantic information and propose a feature-language prompt tuning (FLPT) method to further align the virtual image features and textual features. Thirdly, we train a conditional generative model using the well-aligned virtual image features and corresponding semantic text features, enabling the generation of new classes features and achieve better zero-shot generalization. Our framework has been evaluated on five commonly used benchmarks for generalized ZSL, as well as 11 benchmarks for the base-to-new ZSL. The results demonstrate the superiority and effectiveness of our approach. Our code is available in https://github.com/ylong4/DFZSL.},
author = {Tang, Bowen and Zhang, Jing and Yan, Long and Yu, Qian and Sheng, Lu and Xu, Dong},
doi = {10.1609/aaai.v38i6.28316},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2024_Data-Free Generalized Zero-Shot Learning.pdf:pdf},
issn = {23743468},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {Computer Vision(CV): CV: Language and Vision,Computer Vision(CV): CV: Learning & Optimization for CV,Computer Vision(CV): CV: Multi-modal Vision,Computer Vision(CV): CV: Object Detection & Categorization,Computer Vision(CV): CV: Representation Learning for Vision},
number = {6},
pages = {5108--5117},
title = {{Data-Free Generalized Zero-Shot Learning}},
volume = {38},
year = {2024}
}

@inproceedings{Li2024a,
abstract = {3D point cloud semantic segmentation has a wide range of applications. Recently, weakly supervised point cloud segmentation methods have been proposed, aiming to alleviate the expensive and laborious manual annotation process by leveraging scene-level labels. However, these methods have not effectively exploited the rich geometric information (such as shape and scale) and appearance information (such as color and texture) present in RGB-D scans. Furthermore, current approaches fail to fully leverage the point affinity that can be inferred from the feature extraction network, which is crucial for learning from weak scene-level labels. Additionally, previous work overlooks the detrimental effects of the long-tailed distribution of point cloud data in weakly supervised 3D semantic segmentation. To this end, this paper proposes a simple yet effective scene-level weakly supervised point cloud segmentation method with a newly introduced multi-modality point affinity inference module. The point affinity proposed in this paper is characterized by features from multiple modalities (e.g., point cloud and RGB), and is further refined by normalizing the classifier weights to alleviate the detrimental effects of long-tailed distribution without the need of the prior of category distribution. Extensive experiments on the ScanNet and S3DIS benchmarks verify the effectiveness of our proposed method, which outperforms the state-of-the-art by ∼ 4% to ∼ 6% mIoU. Codes are released at https://github.com/Sunny599/AAAI24-3DWSSG-MMA.},
author = {Li, Xiawei and Xu, Qingyuan and Zhang, Jing and Zhang, Tianyi and Yu, Qian and Sheng, Lu and Xu, Dong},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v38i4.28106},
file = {:Users/jingzhang/Documents/buaa/Publications/2024/AAAI24/3DWeakly/AAAI_Press_Formatting_Instructions_for_Authors_Using_LaTeX.pdf:pdf},
issn = {23743468},
number = {4},
pages = {3216--3224},
title = {{Multi-Modality Affinity Inference for Weakly Supervised 3D Semantic Segmentation}},
volume = {38},
year = {2024}
}
@inproceedings{Xue2024,
abstract = {Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks. In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention. Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations. (I) They adopt pre-trained language models (PLMs) to obtain question representations, while PLMs tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities. (II) They neither emphasize the graph structure between entities nor explicitly model the multi-hop relationship in the graph, which will make it difficult to solve complex multi-hop question answering. To alleviate this problem, we propose a novel Question Calibration and Multi-Hop Modeling (QC-MHM) approach. Specifically, We first calibrate the question representation by fusing the question and the time-constrained concepts in KG. Then, we construct the GNN layer to complete multi-hop message passing. Finally, the question representation is combined with the embedding output by the GNN to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions dataset's complex questions are absolutely improved by 5.1% and 1.2% compared to the best-performing baseline. Moreover, QC-MHM can generate interpretable and trustworthy predictions.},
archivePrefix = {arXiv},
arxivId = {2402.13188},
author = {Xue, Chao and Liang, Di and Wang, Pengfei and Zhang, Jing},
booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v38i17.29903},
eprint = {2402.13188},
file = {:Users/jingzhang/Documents/buaa/Publications/2024/AAAI24/QA/AAAI_Press_Formatting_Instructions_for_Authors_Using_LaTeX__2_ (5).pdf:pdf},
issn = {23743468},
number = {17},
pages = {19332--19340},
title = {{Question Calibration and Multi-Hop Modeling for Temporal Question Answering}},
volume = {38},
year = {2024}
}
@article{10.1145/3700443,
author = {Zhang, Hong and Wan, Jiaxu and Zhang, Jing and Yuan, Ding and Li, XuLiang and Yang, Yifan},
title = {P2FTrack: Multi-object tracking with motion Prior and Feature Posterior},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3700443},
doi = {10.1145/3700443},
abstract = {Multiple object tracking (MOT) has emerged as a crucial component of the rapidly developing computer vision. However, existing multi-object tracking methods often overlook the relationship between features and motion, hindering the ability to strike a performance balance between coupled motion and complex scenes. In this work, we propose a novel end-to-end multi-object tracking method that integrates motion and feature information. To achieve this, we introduce a motion prior generator that transforms motion information into attention masks. Additionally, we leverage prior-posterior fusion multi-head attention to combine the motion-derived priors and attention-based posteriors. Our proposed method is extensively evaluated on MOT17 and DanceTrack datasets through comprehensive experiments and ablation studies, demonstrating state-of-the-art performance in the feature-based method with reasonable speed.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
keywords = {multi-object tracking, prior-posterior fusion, transformer}
}

@inproceedings{Xing2024,
abstract = {Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduces attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to address issues of shape over-smoothing, color over-saturation, limited diversity, and slow convergence of the existing text-to-SVG generation methods by modeling SVGs as distributions of control points and colors. Furthermore, VPSD leverages a reward model to re-weight vector particles, which improves aesthetic appeal and accelerates convergence. Extensive experiments are conducted to validate the effectiveness of SVGDreamer, demonstrating its superiority over baseline methods in terms of editability, visual quality, and diversity. Project page: \href{https://ximinng.github.io/SVGDreamer-project/}{https://ximinng.github.io/SVGDreamer-project/}},
archivePrefix = {arXiv},
arxivId = {2312.16476},
author = {Xing, Ximing and Zhou, Haitao and Wang, Chuang and Zhang, Jing and Xu, Dong and Yu, Qian},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
eprint = {2312.16476},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2024_Xing_SVGDreamer_Text_Guided_SVG_Generation_with_Diffusion_Model_CVPR_2024_paper.pdf:pdf},
pages = {4546--4555},
title = {{SVGDreamer: Text Guided SVG Generation with Diffusion Model}},
url = {http://arxiv.org/abs/2312.16476},
year = {2024}
}
@article{Xing2023,
abstract = {Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates vectorized free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of B{\'{e}}zier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than prior work. The code and demo of DiffSketcher can be found at https://ximinng.github.io/DiffSketcher-project/.},
archivePrefix = {arXiv},
arxivId = {2306.14685},
author = {Xing, Ximing and Wang, Chuang and Zhou, Haitao and Zhang, Jing and Yu, Qian and Xu, Dong},
eprint = {2306.14685},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2023_DiffSketcher-Text Guided Vector Sketch Synthesis through Latent Diffusion Models.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models}},
volume = {36},
year = {2023}
}
@article{Wang2023,
abstract = {Recent research has explored the utilization of pre-trained text-image discriminative models, such as CLIP, to tackle the challenges associated with open-vocabulary semantic segmentation. However, it is worth noting that the alignment process based on contrastive learning employed by these models may unintentionally result in the loss of crucial localization information and object completeness, which are essential for achieving accurate semantic segmentation. More recently, there has been an emerging interest in extending the application of diffusion models beyond text-to-image generation tasks, particularly in the domain of semantic segmentation. These approaches utilize diffusion models either for generating annotated data or for extracting features to facilitate semantic segmentation. This typically involves training segmentation models by generating a considerable amount of synthetic data or incorporating additional mask annotations. To this end, we uncover the potential of generative text-to-image conditional diffusion models as highly efficient open-vocabulary semantic segmenters, and introduce a novel training-free approach named DiffSegmenter. Specifically, by feeding an input image and candidate classes into an off-the-shelf pre-trained conditional latent diffusion model, the cross-attention maps produced by the denoising U-Net are directly used as segmentation scores, which are further refined and completed by the followed self-attention maps. Additionally, we carefully design effective textual prompts and a category filtering mechanism to further enhance the segmentation results. Extensive experiments on three benchmark datasets show that the proposed DiffSegmenter achieves impressive results for open-vocabulary semantic segmentation.},
archivePrefix = {arXiv},
arxivId = {2309.02773},
author = {Wang, Jinglong and Li, Xiawei and Zhang, Jing and Xu, Qingyuan and Zhou, Qin and Yu, Qian and Sheng, Lu and Xu, Dong},
eprint = {2309.02773},
file = {:Users/jingzhang/Documents/buaa/Literature/2024国自然面上/Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter.pdf:pdf},
journal = {arXiv preprint},
title = {{Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter}},
url = {http://arxiv.org/abs/2309.02773},
year = {2023}
}
@inproceedings{Zhao2023,
abstract = {With the emergence of VR and AR, 360° data attracts increasing attention from the computer vision and multimedia communities. Typically, 360° data is projected into 2D ERP (equirectangular projection) images for feature extraction. However, existing methods cannot handle the distortions that result from the projection, hindering the development of 360-data-based tasks. Therefore, in this paper, we propose a Transformer-based model called DATFormer to address the distortion problem. We tackle this issue from two perspectives. Firstly, we introduce two distortion-adaptive modules. The first is a Distortion Mapping Module, which guides the model to pre-adapt to distorted features globally. The second module is a Distortion-Adaptive Attention Block that reduces local distortions on multi-scale features. Secondly, to exploit the unique characteristics of 360° data, we present a learnable relation matrix and use it as part of the positional embedding to further improve performance. Extensive experiments are conducted on three public datasets, and the results show that our model outperforms existing 2D SOD (salient object detection) and 360 SOD methods. The source code is available at https://github.com/yjzhao19981027/DATFormer/.},
author = {Zhao, Yinjie and Zhao, Lichen and Yu, Qian and Sheng, Lu and Zhang, Jing and Xu, Dong},
booktitle = {MM 2023 - Proceedings of the 31st ACM International Conference on Multimedia},
doi = {10.1145/3581783.3612025},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2023_Distortion-aware Transformer in 360° Salient Object Detection.pdf:pdf},
isbn = {9798400701085},
keywords = {360 sod,distortion-adaptive,positional embedding,transformer},
pages = {499--508},
title = {{Distortion-aware Transformer in 360° Salient Object Detection}},
year = {2023}
}
@inproceedings{Xue2023,
abstract = {Transformer-based pre-trained models have achieved great improvements in semantic matching. However, existing models still suffer from insufficient ability to capture subtle differences. The modification, addition and deletion of words in sentence pairs may make it difficult for the model to predict their relationship. To alleviate this problem, we propose a novel Dual Path Modeling Framework to enhance the model's ability to perceive subtle differences in sentence pairs by separately modeling affinity and difference semantics. Based on dual-path modeling framework we design the Dual Path Modeling Network (DPM-Net) to recognize semantic relations. And we conduct extensive experiments on 10 well-studied semantic matching and robustness test datasets, and the experimental results show that our proposed method achieves consistent improvements over baselines.},
author = {Xue, Chao and Liang, Di and Wang, Sirui and Zhang, Jing and Wu, Wei},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP49357.2023.10096590},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2023_Dual_Path_Modeling_for_Semantic_Matching_by_Perceiving_Subtle_Conflicts.pdf:pdf},
isbn = {9781728163277},
issn = {15206149},
keywords = {deep learning,dual path modeling,neural language processing,semantic matching},
pages = {1--5},
publisher = {IEEE},
title = {{Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts}},
year = {2023}
}
@article{Liu2023,
author = {Liu, Lei and Hu, Zhihao and Zhang, Jing},
doi = {10.1109/ICME55011.2023.00342},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2023_PCHM-Net_A_New_Point_Cloud_Compression_Framework_for_Both_Human_Vision_and_Machine_Vision.pdf:pdf},
isbn = {9781665468916},
journal = {2023 IEEE International Conference on Multimedia and Expo (ICME)},
pages = {1997--2002},
publisher = {IEEE},
title = {{PCHM-Net : A New Point Cloud Compression Framework for Both Human Vision and Machine Vision}},
year = {2023}
}
@article{Zhao2023a,
author = {Zhao, Lichen and Cai, Daigang and Zhang, Jing and Sheng, Lu and Xu, Dong and Zheng, Rui and Zhao, Yinjie and Wang, Lipeng and Fan, Xibo},
doi = {10.1109/TCSVT.2022.3229081},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2023_Toward_Explainable_3D_Grounded_Visual_Question_Answering_A_New_Benchmark_and_Strong_Baseline.pdf:pdf},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
number = {6},
pages = {2935--2949},
publisher = {IEEE},
title = {{Toward Explainable 3D Grounded Visual Question Answering : A New Benchmark and Strong Baseline}},
volume = {33},
year = {2023}
}
@inproceedings{Cai2022,
abstract = {Observing that the 3D captioning task and the 3D grounding task contain both shared and complementary information in nature, in this work, we propose a unified framework to jointly solve these two distinct but closely related tasks in a synergistic fashion, which consists of both shared task-agnostic modules and lightweight task-specific modules. On one hand, the shared task-agnostic modules aim to learn precise locations of objects, fine-grained attribute features to characterize different objects, and complex relations between objects, which benefit both caption-ing and visual grounding. On the other hand, by casting each of the two tasks as the proxy task of another one, the lightweight task-specific modules solve the captioning task and the grounding task respectively. Extensive experiments and ablation study on three 3D vision and language datasets demonstrate that our joint training framework achieves significant performance gains for each individual task and finally improves the state-of-the-art performance for both captioning and grounding tasks.},
author = {Cai, Daigang and Zhao, Lichen and Zhang, Jing and Sheng, Lu and Xu, Dong},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/cvpr52688.2022.01597},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2022_Cai_3DJCG_A_Unified_Framework_for_Joint_Dense_Captioning_and_Visual_CVPR_2022_paper.pdf:pdf},
isbn = {9781665469463},
issn = {10636919},
pages = {16443--16452},
title = {{3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds}},
year = {2022}
}
@article{Ye2022,
abstract = {A convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, widely used for analyzing and processing images. Multilayer perceptrons, which we discussed in the previous chapter, usually require fully connected networks, where each neuron in one layer is connected to all neurons in the next layer. Unfortunately, this type of connections inescapably increases the number of weights.},
author = {Ye, Jong Chul},
doi = {10.1007/978-981-16-6046-7_7},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2016_Action_Recognition_From_Depth_Maps_Using_Deep_Convolutional_Neural_Networks.pdf:pdf},
issn = {21983283},
journal = {IEEE Transactions on Human-Machine Systems},
number = {4},
pages = {113--134},
publisher = {IEEE},
title = {{Action Recognition From Depth Maps Using Deep Convolutional Neural Networks}},
volume = {37},
year = {2022}
}
@inproceedings{Wang2022,
abstract = {Point cloud registration aims at estimating the geometric transformation between two point cloud scans, in which point-wise correspondence estimation is the key to its success. In addition to previous methods that seek correspondences by hand-crafted or learnt geometric features, recent point cloud registration methods have tried to apply RGB-D data to achieve more accurate correspondence. However, it is not trivial to effectively fuse the geometric and visual information from these two distinctive modalities, especially for the registration problem. In this work, we propose a new Geometry-Aware Visual Feature Extractor (GAVE) that employs multi-scale local linear transformation to progressively fuse these two modalities, where the geometric features from the depth data act as the geometry-dependent convolution kernels to transform the visual features from the RGB data. The resultant visual-geometric features are in canonical feature spaces with alleviated visual dissimilarity caused by geometric changes, by which more reliable correspondence can be achieved. The proposed GAVE module can be readily plugged into recent RGB-D point cloud registration framework. Extensive experiments on 3D Match and ScanNet demonstrate that our method outperforms the state-of-the-art point cloud registration methods even without correspondence or pose supervision. The code is available at: https://github.com/514DNA/LLT.},
archivePrefix = {arXiv},
arxivId = {2208.14893},
author = {Wang, Ziming and Huo, Xiaoliang and Chen, Zhenghao and Zhang, Jing and Sheng, Lu and Xu, Dong},
booktitle = {European Conference on Computer Vision},
doi = {10.1007/978-3-031-19824-3_11},
eprint = {2208.14893},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2022_Improving RGB-D Point Cloud Registration by Learning Multi-scale Local Linear Transformation.pdf:pdf},
isbn = {9783031198236},
issn = {16113349},
keywords = {2022,Geometric-visual feature extractor,Local linear transformation,Point cloud registration,c the author,equal contributions,extractor,geometric-visual feature,huo,local linear transformation,point cloud registration,s,springer nature switzerland ag,under exclusive license to,wang and x,z},
pages = {175--191},
publisher = {Springer Nature Switzerland},
title = {{Improving RGB-D Point Cloud Registration by Learning Multi-scale Local Linear Transformation}},
url = {http://dx.doi.org/10.1007/978-3-031-19824-3_11},
volume = {1},
year = {2022}
}
@inproceedings{Zhang2021,
abstract = {Recently, machine learning has been widely used for services classification that plays a crucial role in services discovery, selection, and composition. The current methods mostly rely on only one data modality (e.g. services description) for web services classification but fail to fully exploit other readily available data modalities (e.g. services names, and URL). In this paper, a novel MultiModal-Attention-based deep neural network (MMA-Net) is proposed to facilitate the web services classification task via effective feature learning from multiple readily available data modalities. Specifically, a new multimodal feature learning module is introduced to achieve effective message passing and information exchanging among multiple modalities. We conduct experiments on the real-world web services dataset using various evaluation metrics, and the results show that our framework achieves the state-of-the-art results.},
author = {Zhang, Jing and Lei, Changran and Yang, Yilong and Wang, Borui and Chen, Yang},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-91431-8_48},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2021_Zhang2021_Chapter_MMA-NetAMultiModal-Attention-B.pdf:pdf},
isbn = {9783030914301},
issn = {16113349},
keywords = {Attention,Deep learning,Multimodal learning,Services classification,Web services},
pages = {717--727},
publisher = {Springer International Publishing},
title = {{MMA-Net: A MultiModal-Attention-Based Deep Neural Network for Web Services Classification}},
url = {http://dx.doi.org/10.1007/978-3-030-91431-8_48},
volume = {13121 LNCS},
year = {2021}
}
@article{Zhang2021a,
abstract = {In this work, we propose a new generic multi-modality domain adaptation framework called Progressive Modality Cooperation (PMC) to transfer the knowledge learned from the source domain to the target domain by exploiting multiple modality clues (e.g., RGB and depth) under the multi-modality domain adaptation (MMDA) and the more general multi-modality domain adaptation using privileged information (MMDA-PI) settings. Under the MMDA setting, the samples in both domains have all the modalities. Through effective collaboration among multiple modalities, the two newly proposed modules in our PMC can select the reliable pseudo-labeled target samples, which captures the modality-specific information and modality-integrated information, respectively. Under the MMDA-PI setting, some modalities are missing in the target domain. Hence, to better exploit the multi-modality data in the source domain, we further propose the PMC with privileged information (PMC-PI) method by proposing a new multi-modality data generation (MMG) network. MMG generates the missing modalities in the target domain based on the source domain data by considering both domain distribution mismatch and semantics preservation, which are respectively achieved by using adversarial learning and conditioning on weighted pseudo semantic class labels. Extensive experiments on three image datasets and eight video datasets for various multi-modality cross-domain visual recognition tasks under both MMDA and MMDA-PI settings clearly demonstrate the effectiveness of our proposed PMC framework.},
author = {Zhang, Weichen and Xu, Dong and Zhang, Jing and Ouyang, Wanli},
doi = {10.1109/TIP.2021.3052083},
file = {:Users/jingzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2021 - Progressive modality cooperation for multi-modality domain adaptation.pdf:pdf},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
keywords = {Domain adaptation,adversarial learning,deep learning,learning using privileged information (LUPI),multi-modality learning,self-paced learning,transfer learning},
pages = {3293--3306},
pmid = {33481713},
title = {{Progressive Modality Cooperation for Multi-Modality Domain Adaptation}},
volume = {30},
year = {2021}
}
@inproceedings{Zhang2021b,
abstract = {Automatic service classification plays an important role in service discovery, selection, and composition. Recently, machine learning has been widely used in service classification. Though promising results are obtained, previous methods are merely evaluated on web services datasets with small-scale data and relatively balanced data, which limit their real-world applications. In this paper, we address the long-tailed web services classification problem with more categories and imbalanced data. Due to the long-tailed distribution of datasets, the existing machine learning and deep learning methods cannot work well. To deal with the long-tailed problem, we propose a normalized multi-head classifier learning strategy, which effectively reduces the classifier bias and benefit the generalization capacity of the extracted features. Extensive experiments are conducted on a large-scale long-tailed web services dataset, and the results show that our model outperforms the 11 compared service classification methods to a large margin.},
author = {Zhang, Jing and Chen, Yang and Yang, Yilong and Lei, Changran and Wang, Deqiang},
booktitle = {IEEE International Conference on Web Services},
doi = {10.1109/ICWS53863.2021.00025},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2021_ServeNet-LT_A_Normalized_Multi-head_Deep_Neural_Network_for_Long-tailed_Web_Services_Classification.pdf:pdf},
isbn = {9781665416818},
keywords = {Deep Learning,Long-tailed Distributions,Service,Service Classification,Web Services},
pages = {97--106},
publisher = {IEEE},
title = {{ServeNet-LT: A Normalized Multi-head Deep Neural Network for Long-tailed Web Services Classification}},
year = {2021}
}
@inproceedings{Ye2021,
abstract = {Deep\footnote learning-based semantic segmentation methods require a huge amount of training images with pixel-level annotations. Unsupervised domain adaptation (UDA) for semantic segmentation enables transferring knowledge learned from the synthetic data (source domain) with low-cost annotations to the real images (target domain). However, current UDA methods mostly require full access to the source domain data for feasible adaptation, which limits their applications in real-world scenarios with privacy, storage, or transmission issues. To this end, this paper identifies and addresses a more practical but challenging problem of UDA for semantic segmentation, where access to the original source domain data is forbidden. In other words, only the pre-trained source model and unlabelled target domain data are available for adaptation. To tackle the problem, we propose to construct a set of source domain virtual data to mimic the source domain distribution by identifying the target domain high-confidence samples predicted by the pre-trained source model. Then by analyzing the data properties in the cross-domain semantic segmentation tasks, we propose an uncertainty and prior distribution-aware domain adaptation method to align the virtual source domain and the target domain with both adversarial learning and self-training strategies. Extensive experiments on three cross-domain semantic segmentation datasets with in-depth analyses verify the effectiveness of the proposed method.},
author = {Ye, Mucong and Zhang, Jing and Ouyang, Jinpeng and Yuan, DIng},
booktitle = {ACM International Conference on Multimedia},
doi = {10.1145/3474085.3475384},
file = {:Users/jingzhang/Documents/buaa/Literature/SemanticSegmentation/Source Data-Free Unsupervised Domain Adaptation for Semantic Segmentation.pdf:pdf},
isbn = {9781450386517},
keywords = {domain adaptation,semantic segmentation,source data-free},
pages = {2233--2242},
title = {{Source Data-free Unsupervised Domain Adaptation for Semantic Segmentation}},
year = {2021}
}
@inproceedings{Yang2021,
abstract = {Web service classification is one of the common approaches to discover and reuse services. Machine learning methods are widely used for web service classification. However, due to the limited high-quality services in the public dataset, the state-of-the-art deep learning methods can not achieve high accuracy. In this paper, we propose a transfer learning approach Tr-ServeNet to reuse the knowledge of the App classification problem for web service classification. We pre-train a deep learning model for the App classification problem, in which the dataset contains high-quality data from Apple Store, and then transfer the embedded and extracted features to assist web service classification. To demonstrate the effectiveness of our approach, we compare the proposed method with other existing machine learning methods on the 50-category benchmark with 10, 000 real-world web services. The experimental results indicate that the proposed transfer learning method can reach the highest Top-1 accuracy in the benchmark of service classification.},
author = {Yang, Yilong and Li, Zhaotian and Zhang, Jing and Chen, Yang},
booktitle = {IEEE International Conference on Web Services},
doi = {10.1109/ICWS53863.2021.00036},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2021_Transfer_Learning_for_Web_Services_Classification.pdf:pdf},
isbn = {9781665416818},
keywords = {Deep Learning,Service,Service Classification,Service Discovery,Transfer Learning,Web Services},
pages = {185--191},
publisher = {IEEE},
title = {{Transfer Learning for Web Services Classification}},
year = {2021}
}
@inproceedings{Ye2020,
abstract = {Multi-scale feature fusion has been an effective way for improving the performance of semantic segmentation. However, current methods generally fail to consider the semantic gaps between the shallow (low-level) and deep (high-level) features and thus the fusion methods may not be optimal. In this paper, to address the issues of the semantic gap between the feature from different layers, we propose a unified framework based on the U-shape encoder-decoder architecture, named Enhanced Feature Pyramid Network (EFPN). Specifically, the semantic enhancement module (SEM), edge extraction module (EEM), and context aggregation model (CAM) are incorporated into the decoder network to improve the robustness of the multilevel features aggregation. In addition, a global fusion model (GFM), which in the encoder branch is proposed to capture more semantic information in the deep layers and effectively transmit the high-level semantic features to each layer. Extensive experiments are conducted and the results show that the proposed framework achieves the state-of-the-art results on three public datasets, namely PASCAL VOC 2012, Cityscapes, and PASCAL Context. Furthermore, we also demonstrate that the proposed method is effective for other visual tasks that require frequent fusing features and upsampling.},
author = {Ye, Mucong and Ouyang, Jingpeng and Chen, Ge and Zhang, Jing and Yu, Xiaogang},
booktitle = {International Conference on Pattern Recognition},
doi = {10.1109/ICPR48806.2021.9413224},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2020_Enhanced_Feature_Pyramid_Network_for_Semantic_Segmentation.pdf:pdf},
isbn = {9781728188089},
issn = {10514651},
pages = {3209--3216},
title = {{Enhanced feature pyramid network for semantic segmentation}},
year = {2020}
}
@article{Zhang2019,
abstract = {This paper presents a new perspective to formulate unsupervised domain adaptation as a multi-task learning problem. This formulation removes the commonly used assumption in the classifier-based adaptation approach that a shared classifier exists for the same task in different domains. Specifically, the source task is to learn a linear classifier from the labelled source data and the target task is to learn a linear transform to cluster the unlabelled target data such that the original target data are mapped to a lower dimensional subspace where the geometric structure is preserved. The two tasks are jointly learned by enforcing the target transformation is close to the source classifier and the class distribution shift between domains is reduced in the meantime. Two novel classifier-based adaptation algorithms are proposed upon the formulation using Regularized Least Squares and Support Vector Machines respectively, in which unshared classifiers between the source and target domains are assumed and jointly learned to effectively deal with large domain shift. Experiments on both synthetic and real-world cross domain recognition tasks have shown that the proposed methods outperform several state-of-the-art unsupervised domain adaptation methods.},
archivePrefix = {arXiv},
arxivId = {1803.09208},
author = {Zhang, Jing and Li, Wanqing and Ogunbona, Philip},
doi = {10.1016/j.knosys.2019.104975},
eprint = {1803.09208},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2019_Unsupervised domain adaptation- A multi-task learning-based method.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Digit recognition,Object recognition,Transfer learning,Unsupervised domain adaptation},
pages = {104975},
publisher = {Elsevier B.V.},
title = {{Unsupervised domain adaptation: A multi-task learning-based method}},
url = {https://doi.org/10.1016/j.knosys.2019.104975},
volume = {186},
year = {2019}
}
@book{Zhang2018,
abstract = {Human activity understanding from RGB-D data has attracted increasing attention since the first work reported in 2010. Over this period, many benchmark datasets have been created to facilitate the development and evaluation of new algorithms. However, the existing datasets are mostly captured in laboratory environment with small number of actions and small variations, which impede the development of higher level algorithms for real world applications. Thus, this paper proposes a large scale dataset along with a set of evaluation protocols. The large dataset is created by combining several existing publicly available datasets and can be expanded easily by adding more datasets. The large dataset is suitable for testing algorithms from different perspectives using the proposed evaluation protocols. Four state-of-the-art algorithms are evaluated on the large combined dataset and the results have verified the limitations of current algorithms and the effectiveness of the large dataset.},
author = {Zhang, Jing and Li, Wanqing and Wang, Pichao and Ogunbona, Philip and Liu, Song and Tang, Chang},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-91863-1_8},
file = {:Users/jingzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - A large scale RGB-D dataset for action recognition.pdf:pdf;:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2016_A Large Scale RGB-D Dataset for Action Recognition.pdf:pdf},
isbn = {9783319918624},
issn = {16113349},
keywords = {Action recognition,Evaluation protocol,Large scale RGB-D dataset},
number = {D},
pages = {101--114},
publisher = {Springer International Publishing},
title = {{A large scale RGB-D dataset for action recognition}},
url = {http://dx.doi.org/10.1007/978-3-319-91863-1_8},
volume = {10188 LNCS},
year = {2018}
}
@inproceedings{Zhang2018a,
abstract = {This paper proposes an importance weighted adversarial nets-based method for unsupervised domain adaptation, specific for partial domain adaptation where the target domain has less number of classes compared to the source domain. Previous domain adaptation methods generally assume the identical label spaces, such that reducing the distribution divergence leads to feasible knowledge transfer. However, such an assumption is no longer valid in a more realistic scenario that requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. This paper extends the adversarial nets-based domain adaptation and proposes a novel adversarial nets-based partial domain adaptation method to identify the source samples that are potentially from the outlier classes and, at the same time, reduce the shift of shared classes between domains.},
archivePrefix = {arXiv},
arxivId = {1803.09210},
author = {Zhang, Jing and Ding, Zewei and Li, Wanqing and Ogunbona, Philip},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00851},
eprint = {1803.09210},
file = {:Users/jingzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - Importance Weighted Adversarial Nets for Partial Domain Adaptation(2).pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
pages = {8156--8164},
title = {{Importance Weighted Adversarial Nets for Partial Domain Adaptation}},
year = {2018}
}
@inproceedings{Zhang2017,
abstract = {This paper presents a novel unsupervised domain adaptation method for cross-domain visual recognition. We propose a unified framework that reduces the shift between domains both statistically and geometrically, referred to as Joint Geometrical and Statistical Alignment (JGSA). Specifically, we learn two coupled projections that project the source domain and target domain data into lowdimensional subspaces where the geometrical shift and distribution shift are reduced simultaneously. The objective function can be solved efficiently in a closed form. Extensive experiments have verified that the proposed method significantly outperforms several state-of-the-art domain adaptation methods on a synthetic dataset and three different real world cross-domain visual recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1705.05498},
author = {Zhang, Jing and Li, Wanqing and Ogunbona, Philip},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2017.547},
eprint = {1705.05498},
file = {:Users/jingzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Li, Ogunbona - 2017 - Joint Geometrical and Statistical Alignment for Visual Domain Adaptation(2).pdf:pdf},
isbn = {9781538604571},
pages = {5150--5158},
title = {{Joint geometrical and statistical alignment for visual domain adaptation}},
url = {http://arxiv.org/abs/1705.05498},
year = {2017}
}
@article{Wang2016,
abstract = {A convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, widely used for analyzing and processing images. Multilayer perceptrons, which we discussed in the previous chapter, usually require fully connected networks, where each neuron in one layer is connected to all neurons in the next layer. Unfortunately, this type of connections inescapably increases the number of weights.},
author = {Wang, Pichao and Li, Wanqing and Gao, Zhimin and Zhang, Jing and Tang, Chang and Ogunbona, Philip O.},
doi = {10.1109/THMS.2015.2504550},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2016_Action_Recognition_From_Depth_Maps_Using_Deep_Convolutional_Neural_Networks.pdf:pdf},
issn = {21682291},
journal = {IEEE Transactions on Human-Machine Systems},
keywords = {Action recognition,deep learning,depth maps,pseudocolor coding},
number = {4},
pages = {113--134},
publisher = {IEEE},
title = {{Action Recognition from Depth Maps Using Deep Convolutional Neural Networks}},
volume = {46},
year = {2016}
}
@article{Zhang2016,
abstract = {Human action recognition from RGB-D (Red, Green, Blue and Depth) data has attracted increasing attention since the first work reported in 2010. Over this period, many benchmark datasets have been created to facilitate the development and evaluation of new algorithms. This raises the question of which dataset to select and how to use it in providing a fair and objective comparative evaluation against state-of-the-art methods. To address this issue, this paper provides a comprehensive review of the most commonly used action recognition related RGB-D video datasets, including 27 single-view datasets, 10 multi-view datasets, and 7 multi-person datasets. The detailed information and analysis of these datasets is a useful resource in guiding insightful selection of datasets for future research. In addition, the issues with current algorithm evaluation vis-\'{a}-vis limitations of the available datasets and evaluation protocols are also highlighted; resulting in a number of recommendations for collection of new datasets and use of evaluation protocols.},
archivePrefix = {arXiv},
arxivId = {1601.05511},
author = {Zhang, Jing and Li, Wanqing and Ogunbona, Philip O. and Wang, Pichao and Tang, Chang},
doi = {10.1016/j.patcog.2016.05.019},
eprint = {1601.05511},
file = {:Users/jingzhang/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2016 - RGB-D-based Action Recognition Datasets A Survey(4).pdf:pdf},
issn = {00313203},
journal = {arXiv preprint arXiv:1601.05511v1},
keywords = {action recognition,evaluation protocol,rgb-d dataset},
number = {Wanqing Li},
pages = {40},
title = {{RGB-D-based Action Recognition Datasets: A Survey}},
url = {http://arxiv.org/abs/1601.05511},
volume = {1},
year = {2016}
}
@inproceedings{Wang2015,
abstract = {In this paper, we propose to adopt ConvNets to recognize human actions from depth maps on relatively small datasets based on Depth Motion Maps (DMMs). In particular, three strategies are developed to effectively leverage the capability of ConvNets in mining discriminative features for recognition. Firstly, different viewpoints are mimicked by rotating virtual cameras around subject represented by the 3D points of the captured depth maps. This not only synthesizes more data from the captured ones, but also makes the trained ConvNets view-Tolerant. Secondly, DMMs are constructed and further enhanced for recognition by encoding them into Pseudo-RGB images, turning the spatial-Temporal motion patterns into textures and edges. Lastly, through transferring learning the models originally trained over ImageNet for image classification, the three ConvNets are trained independently on the colorcoded DMMs constructed in three orthogonal planes. The proposed algorithm was extensively evaluated on MSRAction3D, MSRAction3DExt and UTKinect-Action datasets and achieved the stateof-the-Art results on these datasets.},
author = {Wang, Pichao and Li, Wanqing and Gao, Zhimin and Tang, Chang and Zhang, Jing and Ogunbona, Philip},
booktitle = {Proceedings of the 2015 ACM Multimedia Conference},
doi = {10.1145/2733373.2806296},
file = {:Users/jingzhang/Documents/buaa/基金项目/个人成果/个人成果原文/2015_ConvNets-Based Action Recognition from Depth Maps through Virtual Cameras and Pseudocoloring.pdf:pdf},
isbn = {9781450334594},
keywords = {Action Recognition,ConvNets,Pseudocoloring,Virtual Cameras},
pages = {1119--1122},
title = {{Convnets-based action recognition from depth maps through virtual cameras and pseudocoloring}},
year = {2015}
}

